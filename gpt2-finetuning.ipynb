{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7961276,"sourceType":"datasetVersion","datasetId":4670170}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import shutil\n\n# Original dataset path\noriginal_dataset_path = \"/kaggle/input/medqa-senior/dataset.txt\"\n\n# Destination path in the working directory\ndestination_path = \"/kaggle/working/dataset_copy.txt\"\n\n# Copy the contents of the original dataset file to the destination file\nshutil.copyfile(original_dataset_path, destination_path)\n\n# Now you can use the copied dataset file for training\ndataset_path = destination_path\n","metadata":{"execution":{"iopub.status.busy":"2024-03-28T03:20:00.620266Z","iopub.execute_input":"2024-03-28T03:20:00.621145Z","iopub.status.idle":"2024-03-28T03:20:00.627762Z","shell.execute_reply.started":"2024-03-28T03:20:00.621103Z","shell.execute_reply":"2024-03-28T03:20:00.626864Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n\n# Load pre-trained GPT-2 model and tokenizer\nmodel_name = \"gpt2-medium\"  # or any other pre-trained model\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\n\n# Load domain-specific dataset\nprint(dataset_path)\ndataset = TextDataset(\n    tokenizer=tokenizer,\n    file_path=dataset_path,\n    block_size=128  # Adjust block size according to your dataset\n)\n\n# Prepare data collator\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./output\",\n    overwrite_output_dir=True,\n    num_train_epochs=3,  # Adjust number of epochs as needed\n    per_device_train_batch_size=4,  # Adjust batch size as needed\n    save_steps=10_000,\n    save_total_limit=2,\n)\n\n# Create Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=dataset,\n)\n\n# Fine-tune the model on the domain-specific dataset\ntrainer.train()\n\n# Save the fine-tuned model\nmodel.save_pretrained(\"./fine_tuned_model\")\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-28T03:20:20.779720Z","iopub.execute_input":"2024-03-28T03:20:20.780081Z","iopub.status.idle":"2024-03-28T03:21:54.050342Z","shell.execute_reply.started":"2024-03-28T03:20:20.780052Z","shell.execute_reply":"2024-03-28T03:21:54.049364Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/kaggle/working/dataset_copy.txt\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240328_032108-eozcfeov</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/krbala1511/huggingface/runs/eozcfeov' target=\"_blank\">bright-yogurt-2</a></strong> to <a href='https://wandb.ai/krbala1511/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/krbala1511/huggingface' target=\"_blank\">https://wandb.ai/krbala1511/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/krbala1511/huggingface/runs/eozcfeov' target=\"_blank\">https://wandb.ai/krbala1511/huggingface/runs/eozcfeov</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:05, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer\nimport textwrap\n\n# Load fine-tuned GPT-2 model and tokenizer\nmodel_path = \"./fine_tuned_model\"  # Path to your fine-tuned model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\nmodel = GPT2LMHeadModel.from_pretrained(model_path)\n\n# Define a function to generate responses to questions\ndef generate_response(question, max_length=50):\n    input_text = question\n    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n    response = tokenizer.decode(output[0], skip_special_tokens=True)\n    return response\n\n# Example usage\nquestion = \"What is Alzheimer's disease?\"\nresponse = generate_response(question)\nresponse = response[:response.rfind('.')+1]\nprint(\"Response:\", textwrap.fill(response,width = 80))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}