{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":8113172,"datasetId":4670170,"databundleVersionId":8231789}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import csv\n\n# Define the path to your CSV file\ncsv_file_path = \"/kaggle/input/medqa-senior/dataset.csv\"\n\n# Define the columns you want to read\ncolumns_to_read = [\"question\", \"answer\"]\n\n# Initialize lists to store data from each column\ndata = {col: [] for col in columns_to_read}\n\n# Open the CSV file and read the specified columns\nwith open(csv_file_path, \"r\", newline=\"\") as csvfile:\n    reader = csv.DictReader(csvfile)\n    for row in reader:\n        for col in columns_to_read:\n            data[col].append(row[col])\nfor i in data[\"answer\"]:\n    print(len(i))","metadata":{"execution":{"iopub.status.busy":"2024-04-14T09:42:33.425734Z","iopub.execute_input":"2024-04-14T09:42:33.426046Z","iopub.status.idle":"2024-04-14T09:42:33.433740Z","shell.execute_reply.started":"2024-04-14T09:42:33.426020Z","shell.execute_reply":"2024-04-14T09:42:33.432839Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"129\n103\n113\n150\n100\n151\n188\n99\n154\n140\n203\n155\n166\n162\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\nfrom torch.utils.data import Dataset, DataLoader\n\n# Define your closed-book QA dataset class\nclass ClosedBookQADataset(Dataset):\n    def __init__(self, questions, answers):\n        self.questions = questions\n        self.answers = answers\n\n    def __len__(self):\n        return len(self.questions)\n\n    def __getitem__(self, idx):\n        return {\"question\": self.questions[idx], \"answer\": self.answers[idx]}\n\n# Define your model and tokenizer\ntokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\n# Prepare your dataset (replace this with your actual dataset)\nquestions = data[\"question\"]  # List of questions\nanswers = data[\"answer\"]    # List of corresponding answers\ndataset = ClosedBookQADataset(questions, answers)\n\n# Define training parameters\nepochs = 3\nbatch_size = 4\nlearning_rate = 3e-4\n\n# Prepare DataLoader\ntrain_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Training loop\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for batch in train_loader:\n        inputs = tokenizer(batch[\"question\"], return_tensors=\"pt\", padding=True, truncation=True)\n        labels = tokenizer(batch[\"answer\"], return_tensors=\"pt\", padding=True, truncation=True)\n\n        # Forward pass\n        outputs = model(**inputs, labels=labels[\"input_ids\"])\n\n        # Compute loss\n        loss = outputs.loss\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch + 1}: Average Loss: {total_loss / len(train_loader)}\")\n\n# Save the trained model\nmodel.save_pretrained(\"./model\")\ntokenizer.save_pretrained(\"./model\")","metadata":{"execution":{"iopub.status.busy":"2024-04-14T09:49:39.200248Z","iopub.execute_input":"2024-04-14T09:49:39.200657Z","iopub.status.idle":"2024-04-14T09:49:48.634335Z","shell.execute_reply.started":"2024-04-14T09:49:39.200619Z","shell.execute_reply":"2024-04-14T09:49:48.633274Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Average Loss: 5.583241939544678\nEpoch 2: Average Loss: 3.6626630425453186\nEpoch 3: Average Loss: 2.972608119249344\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"('./model/tokenizer_config.json',\n './model/special_tokens_map.json',\n './model/spiece.model',\n './model/added_tokens.json')"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\n\n# Load the trained model and tokenizer\nmodel_path = \"/kaggle/working/model\"  # Update with the path where your model is saved\ntokenizer = T5Tokenizer.from_pretrained(model_path)\nmodel = T5ForConditionalGeneration.from_pretrained(model_path)\n\n# Define a function to generate answers for questions\ndef generate_answer(question):\n    # Prepare input for the model\n    input_text = f\"question: {question}\"\n    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n\n    # Generate answer\n    with torch.no_grad():\n        output_ids = model.generate(input_ids)\n    \n    # Decode and return the answer\n    answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    return answer\n\n# Example usage\nquestion = \"What is Alzheimer's disease?\"\nanswer = generate_answer(question)\nprint(\"Answer:\", answer)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T09:50:16.587445Z","iopub.execute_input":"2024-04-14T09:50:16.588121Z","iopub.status.idle":"2024-04-14T09:50:17.238592Z","shell.execute_reply.started":"2024-04-14T09:50:16.588090Z","shell.execute_reply":"2024-04-14T09:50:17.237680Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"name":"stdout","text":"Answer: Alzheimer's disease\n","output_type":"stream"}]}]}